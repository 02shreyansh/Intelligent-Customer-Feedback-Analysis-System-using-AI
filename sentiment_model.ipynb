{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4eccc0c0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/workspaces/Intelligent-Customer-Feedback-Analysis-System-using-AI/myenv/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "from transformers import DistilBertTokenizer, DistilBertForSequenceClassification, Trainer, TrainingArguments\n",
    "from datasets import Dataset, load_dataset\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, precision_recall_fscore_support, classification_report\n",
    "import pickle\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "fce625e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('dataset/csv/processed_data.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "043e6696",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(42)\n",
    "df['sentiment'] = df['original_label'].copy()\n",
    "sentiment_map = {0: 'Negative', 1: 'Positive'}\n",
    "df['sentiment_label'] = df['sentiment'].map(sentiment_map)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "38e83670",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df, test_df = train_test_split(df, test_size=0.2, random_state=42, stratify=df['sentiment'])\n",
    "train_df, val_df = train_test_split(train_df, test_size=0.1, random_state=42, stratify=train_df['sentiment'])\n",
    "tokenizer = DistilBertTokenizer.from_pretrained('distilbert-base-uncased')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9527f25e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_function(examples):\n",
    "    return tokenizer(examples['text'], padding='max_length', truncation=True, max_length=128)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6ab1a569",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = Dataset.from_dict({\n",
    "    'text': train_df['processed_text'].tolist(),\n",
    "    'label': train_df['sentiment'].tolist()\n",
    "})\n",
    "val_dataset = Dataset.from_dict({\n",
    "    'text': val_df['processed_text'].tolist(),\n",
    "    'label': val_df['sentiment'].tolist()\n",
    "})\n",
    "test_dataset = Dataset.from_dict({\n",
    "    'text': test_df['processed_text'].tolist(),\n",
    "    'label': test_df['sentiment'].tolist()\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6322346b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 1080/1080 [00:00<00:00, 1150.95 examples/s]\n",
      "Map: 100%|██████████| 120/120 [00:00<00:00, 1250.62 examples/s]\n",
      "Map: 100%|██████████| 300/300 [00:00<00:00, 1265.46 examples/s]\n"
     ]
    }
   ],
   "source": [
    "train_dataset = train_dataset.map(tokenize_function, batched=True)\n",
    "val_dataset = val_dataset.map(tokenize_function, batched=True)\n",
    "test_dataset = test_dataset.map(tokenize_function, batched=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "397c9571",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight', 'pre_classifier.bias', 'pre_classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "model = DistilBertForSequenceClassification.from_pretrained(\n",
    "    'distilbert-base-uncased',\n",
    "    num_labels=2\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7b95ccde",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_metric(pred):\n",
    "    labels = pred.label_ids\n",
    "    preds = pred.predictions.argmax(-1)\n",
    "    precision, recall, f1, _ = precision_recall_fscore_support(labels, preds, average='weighted')\n",
    "    acc = accuracy_score(labels, preds)\n",
    "    return {\n",
    "        'accuracy': acc,\n",
    "        'f1': f1,\n",
    "        'precision': precision,\n",
    "        'recall': recall\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "6540c36a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='136' max='136' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [136/136 15:54, Epoch 2/2]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>F1</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.675076</td>\n",
       "      <td>0.641667</td>\n",
       "      <td>0.591998</td>\n",
       "      <td>0.792729</td>\n",
       "      <td>0.641667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.673900</td>\n",
       "      <td>0.440777</td>\n",
       "      <td>0.783333</td>\n",
       "      <td>0.780771</td>\n",
       "      <td>0.794467</td>\n",
       "      <td>0.783333</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=136, training_loss=0.6332800248089958, metrics={'train_runtime': 963.1028, 'train_samples_per_second': 2.243, 'train_steps_per_second': 0.141, 'total_flos': 71532395274240.0, 'train_loss': 0.6332800248089958, 'epoch': 2.0})"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "training_args = TrainingArguments(\n",
    "    output_dir='results',\n",
    "    num_train_epochs=2,\n",
    "    per_device_train_batch_size=16,\n",
    "    per_device_eval_batch_size=16,\n",
    "    warmup_steps=500,\n",
    "    weight_decay=0.01,\n",
    "    logging_dir='logs',\n",
    "    logging_steps=100,\n",
    "    save_strategy=\"epoch\",\n",
    "    eval_strategy=\"epoch\",  \n",
    "    load_best_model_at_end=True,\n",
    ")\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=val_dataset,\n",
    "    compute_metrics=compute_metric,\n",
    ")\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "882e749f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "predictions = trainer.predict(test_dataset)\n",
    "preds = predictions.predictions.argmax(-1)\n",
    "labels = predictions.label_ids\n",
    "accuracy = accuracy_score(labels, preds)\n",
    "precision, recall, f1, _ = precision_recall_fscore_support(labels, preds, average='weighted')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "d3a0f08a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.8400\n",
      "Precision: 0.8536\n",
      "Recall: 0.8400\n",
      "F1 Score: 0.8381\n"
     ]
    }
   ],
   "source": [
    "print(f\"Accuracy: {accuracy:.4f}\")\n",
    "print(f\"Precision: {precision:.4f}\")\n",
    "print(f\"Recall: {recall:.4f}\")\n",
    "print(f\"F1 Score: {f1:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "47b9a487",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "    Negative       0.79      0.94      0.86       153\n",
      "    Positive       0.92      0.73      0.82       147\n",
      "\n",
      "    accuracy                           0.84       300\n",
      "   macro avg       0.85      0.84      0.84       300\n",
      "weighted avg       0.85      0.84      0.84       300\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(labels, preds, target_names=['Negative', 'Positive']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "7a7ac59c",
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics_df = pd.DataFrame({\n",
    "    'Metric': ['Accuracy', 'Precision', 'Recall', 'F1 Score'],\n",
    "    'Value': [accuracy, precision, recall, f1]\n",
    "})\n",
    "metrics_df.to_csv('dataset/csv/evaluation_metrics.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "ae46b426",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('sentiment_model/tokenizer_config.json',\n",
       " 'sentiment_model/special_tokens_map.json',\n",
       " 'sentiment_model/vocab.txt',\n",
       " 'sentiment_model/added_tokens.json')"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.save_pretrained('sentiment_model')\n",
    "tokenizer.save_pretrained('sentiment_model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "b028da75",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_info = {\n",
    "    'model_path': 'sentiment_model',\n",
    "    'tokenizer': tokenizer,\n",
    "    'label_map': {0: 'Negative', 1: 'Positive'},\n",
    "    'metrics': {\n",
    "        'accuracy': accuracy,\n",
    "        'precision': precision,\n",
    "        'recall': recall,\n",
    "        'f1_score': f1\n",
    "    }\n",
    "}\n",
    "\n",
    "with open('sentiment_model.pkl', 'wb') as f:\n",
    "    pickle.dump(model_info, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "00318ca9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_sentiment(text):\n",
    "    inputs = tokenizer(text, return_tensors=\"pt\", padding=True, truncation=True, max_length=128)\n",
    "    outputs = model(**inputs)\n",
    "    prediction = torch.argmax(outputs.logits, dim=1).item()\n",
    "    label_map = {0: 'Negative', 1: 'Positive'}\n",
    "    return label_map[prediction]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "37cc6355",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted Sentiment: Positive\n",
      "Predicted Sentiment: Negative\n",
      "Predicted Sentiment: Positive\n"
     ]
    }
   ],
   "source": [
    "test_texts = [\n",
    "    \"This product is amazing! I love it!\",\n",
    "    \"Terrible quality, waste of money.\",\n",
    "    \"It's okay, nothing special.\"\n",
    "]\n",
    "for text in test_texts:\n",
    "    sentiment = predict_sentiment(text)\n",
    "    print(f\"Predicted Sentiment: {sentiment}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
